{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import statement should bring in all the functions you need!\n",
    "import pandas as pd\n",
    "from torch import tensor,sigmoid\n",
    "from Utils.CommonFunctions import NormalScaling,MinMaxScaling\n",
    "from torch.nn import Linear, MSELoss, BCELoss\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This wrapper class is used to make training and extracting the results of that training easier\n",
    "class GlmModelWraper:\n",
    "    \n",
    "    def __init__(self, model, x_df: pd.DataFrame, y_df: pd.DataFrame, learning_rate = .001):\n",
    "        self.Model = model(x_df.shape[1], learning_rate = learning_rate)\n",
    "        self.X = x_df\n",
    "        self.Y = y_df\n",
    "\n",
    "    def train_model(self,num_epochs = 100):\n",
    "        x = tensor(self.X.values).float()\n",
    "        y = tensor(self.Y.values).float()\n",
    "        self.Model.train(x, y, num_epochs)\n",
    "        \n",
    "    def GetFeatureWeights(self):\n",
    "        weights, bias = self.Model.GetFeatureWeights()\n",
    "        param_names = self.X.columns.to_list() + ['bias']\n",
    "        param_vals = weights.flatten().tolist() + bias.flatten().tolist()\n",
    "        return list(zip(param_names, param_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel:\n",
    "    def __init__(self, numFeatures, learning_rate = .001):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    def train(self, x, y, num_epochs = 10):\n",
    "        pass\n",
    "\n",
    "    def GetFeatureWeights(self):\n",
    "        layer = ???\n",
    "        return layer.weight, layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.DatasetGenerator import GenerateSimpleDataSet_OLS\n",
    "df_ols = GenerateSimpleDataSet_OLS(1000)\n",
    "\n",
    "wrapper= GlmModelWraper(LinearRegressionModel, df_ols.drop('Y',axis=1), df_ols[['Y']], learning_rate=.001)\n",
    "wrapper.train_model(500)\n",
    "wrapper.GetFeatureWeights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel:\n",
    "    def __init__(self, numFeatures, learning_rate = .001):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    def train(self, x, y, num_epochs = 10):\n",
    "        pass\n",
    "\n",
    "    def GetFeatureWeights(self):\n",
    "        layer = ???\n",
    "        return layer.weight, layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.DatasetGenerator import GenerateSimpleDataSet_Bernoulli\n",
    "df_ols = GenerateSimpleDataSet_Bernoulli(1000)\n",
    "\n",
    "wrapper= GlmModelWraper(LogisticRegressionModel, df_ols.drop('Y',axis=1), df_ols[['Y']], learning_rate=.001)\n",
    "wrapper.train_model(500)\n",
    "wrapper.GetFeatureWeights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Not all variables belong in a model. In fact sometimes models have too many parameters and that results in issues where we overfit the dataset. To address this and preserve generalization we use regularization. In this section you will implement two regularization methods and add them to your general linear models then observe the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.DatasetGenerator import GenerateSimpleDataSet_Regularization\n",
    "df_regularization = GenerateSimpleDataSet_Regularization(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confounders, Colliders, and Mediators, Oh My!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Confounders\n",
    "A confounder is a variable that influences both the independent and dependent variable in a study resulting in a spurious association between them. Adjusting for confounders can help recover the model's ability to appropriately weigh the effect of a specific independent variable on the outcome of interest.\n",
    "\n",
    "In this dataset there is one confounder that influences the \"independent variable\" A and the outcome variable Y. Use your OLS model framework to try a variety of tactics to prevent the confounder from damaging this model's predictive capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.DatasetGenerator import GenerateSimpleDataSet_Confounder\n",
    "df_confounder = GenerateSimpleDataSet_Confounder(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognizing Colliders\n",
    "A collider is a variable that is influenced by both the independent and dependent variable in a study resulting in a spurious association between them. Are these variables problematic in maintaining the correctness of the model? If so, what can we do to account for them, should we account for them?\n",
    "\n",
    "In this dataset there is one collider that is influenced by the independent variable A and the outcome variable Y. Use your OLS model framework to try a variety of tactics to prevent the colliders from damaging this model's predictive capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.DatasetGenerator import GenerateSimpleDataSet_Collider\n",
    "df_confounder = GenerateSimpleDataSet_Collider(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Mediators\n",
    "A mediator is a variable that is influenced by an independent variable that in turn goes on to impact a dependent variable of the study. This causal path can be very informative, but is often difficult to pin down with a simple linear model. Can we identify the mediator here using our models alone? What are some plans we can use to identify the mediator in more challenging datasets.\n",
    "\n",
    "In this dataset ther is one mediator that is influenced by the independent variable A and goes on to directly impact the outcome variable Y. Use your OLS model framework to see if you can identify characteristics of the mediator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.DatasetGenerator import GenerateSimpleDataSet_Mediator\n",
    "df_confounder = GenerateSimpleDataSet_Mediator(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for a Challenge?\n",
    "Identify the important variables for this analysis and train some form of GLM model to determine when I go on bikerides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.DatasetGenerator import GenerateComplexDataset_BrendansBikeRides\n",
    "df_challenge = GenerateComplexDataset_BrendansBikeRides(5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
